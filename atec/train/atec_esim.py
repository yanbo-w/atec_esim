# -*- coding: utf-8 -*-
"""Atec_ESIM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_fFNZvuHom2Z9ifi1apEzhPL57TWho7
"""

import os
import codecs

path = ''
raw_samples = []
with codecs.open(os.path.join(path,'atec_nlp_sim_train.csv'), 'r', encoding='utf-8') as f:
  for line in f:
    line = line.strip()
    lines = line.split('\t')
    raw_samples.append(lines[1:])
with codecs.open(os.path.join(path,'atec_nlp_sim_train_add.csv'), 'r', encoding='utf-8') as f:
  for line in f:
    line = line.strip()
    lines = line.split('\t')
    raw_samples.append(lines[1:])

import jieba


token_sample = []

words_table = {'[unk]':0}

# jieba.suggest_freq('花呗', True)
jieba.load_userdict(os.path.join(path,'my_dict.txt'))
# jieba.add_word('花呗','借呗','蚂蚁花呗','蚂蚁借呗')

for sample in raw_samples:
  # print('-------------')
  token_list1 = []
  token_list2 = []
  raw_list1 = jieba.lcut(sample[0], cut_all=False, HMM=False)
  raw_list2 = jieba.lcut(sample[1], cut_all=False, HMM=False)
  for word in raw_list1:
    # print(word)
    if word not in words_table.keys():
      words_table[word] = len(words_table)
    token_list1.append(words_table[word])
  # print('%%%%%')
  for word in raw_list2:
    # print(word)
    if word not in words_table.keys():
      words_table[word] = len(words_table)
    token_list2.append(words_table[word])
  token_sample.append([token_list1, token_list2, int(sample[2])])


with codecs.open(os.path.join(path,'vocab.txt'), 'w', encoding='utf-8') as f:
  vocab_table = {v:k for k,v in words_table.items()}
  for rank in range(len(vocab_table)):
    f.write(vocab_table[rank])
    f.write('\n')

import tensorflow as tf
from tensorflow.python.keras.layers import *
from tensorflow.python.keras import *

def align(input_1, input_2):
    attention = Dot(axes=-1)([input_1, input_2])
    w_att_1 = Lambda(lambda x: activations.softmax(x, axis=1))(attention)
    w_att_2 = Permute((2,1))(Lambda(lambda x: activations.softmax(x, axis=2))(attention))
    in1_aligned = Dot(axes=1)([w_att_1, input_1])
    in2_aligned = Dot(axes=1)([w_att_2, input_2])
    return in1_aligned, in2_aligned

def subtract(input_1, input_2):
    minus_input_2 = Lambda(lambda x: -x)(input_2)
    return add([input_1, minus_input_2])

def get_model(seq_len, dict_len, embedding_dim, num_lstm, rate_drop_dense, num_dense):
 
    sequence_1_input = Input(shape=(seq_len,), dtype='int32')
    sequence_2_input = Input(shape=(seq_len,), dtype='int32')
 
    # embedding
    embedding_layer = Embedding(dict_len, embedding_dim, input_length=seq_len)
    bn = BatchNormalization(axis=2)
    embedded_sequences_1 = bn(embedding_layer(sequence_1_input))
    embedded_sequences_2 = bn(embedding_layer(sequence_2_input))
 
    # encode
    encode = Bidirectional(LSTM(num_lstm, return_sequences=True))
    encode_sequences_1 = encode(embedded_sequences_1)
    encode_sequences_2 = encode(embedded_sequences_2)
 
    # attention
    alignd_sequences_1, alignd_sequences_2 = align(encode_sequences_1, encode_sequences_2)
 
    # compose
    combined_sequences_1 = Concatenate()(
        [encode_sequences_1, alignd_sequences_2, subtract(encode_sequences_1, alignd_sequences_2), multiply([encode_sequences_1, alignd_sequences_2])])
    combined_sequences_2 = Concatenate()(
        [encode_sequences_2, alignd_sequences_1, subtract(encode_sequences_2, alignd_sequences_1), multiply([encode_sequences_2, alignd_sequences_1])])
 
    compose = Bidirectional(LSTM(num_lstm, return_sequences=True))
    compare_sequences_1 = compose(combined_sequences_1)
    compare_sequences_2 = compose(combined_sequences_2)
 
    # aggregate
    rep_sequences_1 = concatenate([GlobalAvgPool1D()(compare_sequences_1), GlobalMaxPool1D()(compare_sequences_1)])
    rep_sequences_2 = concatenate([GlobalAvgPool1D()(compare_sequences_2), GlobalMaxPool1D()(compare_sequences_2)])

    # classifier
    merged = Concatenate()([rep_sequences_1, rep_sequences_2])
    dense = BatchNormalization()(merged)
    dense = Dense(num_dense, activation='elu')(dense)
    dense = BatchNormalization()(dense)
    dense = Dropout(rate_drop_dense)(dense)
    dense = Dense(num_dense, activation='elu')(dense)
    dense = BatchNormalization()(dense)
    dense = Dropout(rate_drop_dense)(dense)
    out_ = Dense(1, activation='sigmoid')(dense)
 
    model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=out_)
    model.compile(optimizer=tf.train.AdamOptimizer(), loss='binary_crossentropy', metrics=['binary_crossentropy', 'accuracy'])
    return model

model = get_model(32,len(words_table),256,64,0.3,32)
model.summary()

import numpy as np
data1 = []
data2 = []
label = []
for sample in token_sample:
  input1 = np.zeros(32)
  input2 = np.zeros(32)
  if len(sample[0]) > 32:
    input1 = sample[0][:32]
  else:
    input1[:len(sample[0])] = sample[0]
  if len(sample[1]) > 32:
    input2 = sample[1][:32]
  else:
    input2[:len(sample[1])] = sample[1]
  data1.append(input1)
  data2.append(input2)
  label.append(sample[2])

dev_data1 = []
dev_data2 = []
dev_label = []
train_data1 = []
train_data2 = []
train_label = []

import random
for lab in range(len(label)):
  if random.random() >0.2:
    train_data1.append(data1[lab])
    train_data2.append(data2[lab])
    train_label.append(label[lab])
  else:
    dev_data1.append(data1[lab])
    dev_data2.append(data2[lab])
    dev_label.append(label[lab])

print(train_data1[0])

train_data1 = np.array(train_data1)
train_data2 = np.array(train_data2)
dev_data1 = np.array(dev_data1)
dev_data2 = np.array(dev_data2)

tf.test.gpu_device_name()

class_weight = {0:0.2, 1:0.8}

model.fit([train_data1,train_data2],
          train_label,
          epochs=10,
          batch_size=64,
          validation_data=([dev_data1, dev_data2], dev_label),
          class_weight=class_weight)

model.save(os.path.join(path,'atec_model.h5'))

dev_predict_label = model.predict([dev_data1, dev_data2], batch_size=32)

dev_predict_value = dev_predict_label
dev_predict_label = []
for i in dev_predict_value:
  if i >0.5:
    dev_predict_label.append(1)
  else:
    dev_predict_label.append(0)

from sklearn import metrics
print(metrics.classification_report(dev_label, dev_predict_label))

